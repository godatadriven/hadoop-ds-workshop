{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data processing with Apache Spark\n",
    "\n",
    "In this workshop, we will use the Apache Spark framework for performing computations on large data sets. \n",
    "\n",
    "## What is Spark?\n",
    "\n",
    "From the website:\n",
    "\n",
    "> Apache Spark is a fast and general-purpose cluster computing system. It provides high-level APIs in Java, Scala, Python and R, and an optimized engine that supports general execution graphs. It also supports a rich set of higher-level tools including [Spark SQL](https://spark.apache.org/docs/latest/sql-programming-guide.html) for SQL and structured data processing, [MLlib](https://spark.apache.org/docs/latest/mllib-guide.html) for machine learning, [GraphX](https://spark.apache.org/docs/latest/graphx-programming-guide.html) for graph processing, and [Spark Streaming](https://spark.apache.org/docs/latest/streaming-programming-guide.html).\n",
    "\n",
    "Spark allows you to write reasonably concise, functional style code against large collections of data, which gets executed on a cluster of machines in a scalable way. This means that Spark abstracts the work of taking the code and distributing it over multiple machines and provides a clean API to work with data.\n",
    "\n",
    "## How does it work?\n",
    "In Spark, data is represented as a Resilient Distributed Dataset, or RDD. An RDD is very similar to a collection in normal programming; think of a [List in Scala](http://www.scala-lang.org/api/2.11.5/index.html#scala.collection.immutable.List), a [Stream in Java8](https://docs.oracle.com/javase/8/docs/api/java/util/stream/package-summary.html) or a [list in Python](https://docs.python.org/2/tutorial/datastructures.html). You can work with lists by applying functions to them, filtering them or grouping and aggregating the list items. Just as with normal lists, Spark RDDs allow you to perform all of these operations; however, keep in mind that the execution of the operations will be distributed across a cluster. Let's look at some examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing RDDs\n",
    "There are two ways to create an RDD:\n",
    "\n",
    "1. Create it from a list of data that's currently in memory.\n",
    "2. Read it from a file.\n",
    "\n",
    "Creating an RDD from some in-memory list is not very common and is normally only used for debugging or showing example use cases.\n",
    "\n",
    "Example of creating an RDD from memory:\n",
    "\n",
    "```python\n",
    "rdd = sc.parallelize(range(10000))  # Creates an RDD of the numbers 0...9999\n",
    "```\n",
    "\n",
    "Normally, it makes more sense to read an RDD from a file. Spark has built-in support for several types of files, including text files, JSON and certain types of binary files that are common in the Hadoop ecosystem. When reading data from files, those files will usually be stored on your cluster on a distributed filesystem, such as the [Hadoop Filesystem (HDFS)](http://hadoop.apache.org). This means that when loading data from a file, this data is already distributed on your cluster; Spark will make use of this fact and try to execute the code on the cluster nodes where the data is also located.\n",
    "\n",
    "```python\n",
    "# Creates an RDD from the given text file. Each line in the file becomes a string element in the RDD.\n",
    "# The text file can be on the local filesystem or on some distributed filesystem.\n",
    "rdd = sc.textFile('/data/huge-file.txt')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's create a Spark RDD and a normal Python list.\n",
    "spark_rdd = sc.parallelize(range(100))\n",
    "python_list = range(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with RDDs\n",
    "An RDD is an object that abstracts a distributed dataset. A method call on the RDD object will peform the actual computation across the cluster that Spark is running on, yet it looks like normal collection operations on a local dataset.\n",
    "\n",
    "### Types of operations\n",
    "Spark RDDs have two types of operations:\n",
    "- transformations: these create a new RDD by transforming the existing RDD according to some function (e.g. `map` or `flatMap`)\n",
    "- actions: these perform an action and return the result to the client (usually called driver program).\n",
    "\n",
    "The most common transformation operations are:\n",
    "- `filter(function)`: filter the RDD for elements where function returns True.\n",
    "- `map(function)`: transform each element of the RDD using a function.\n",
    "- `flatMap(function)`: transform each element of the RDD into zero or more elements using function.\n",
    "- `keyBy(...)`: transform the RDD into the same RDD with a key for each element (keys need not be unique).\n",
    "- `groupByKey()`: transform the RDD into an RDD of keys and groups of values with the same key.\n",
    "- `reduceByKey(function)`: group the RDD by key and perform a reduce with a binary function on each group.\n",
    "- `sample(...)`: create a new RDD by sampling a subset of the original RDD; useful for quickly getting an overview of datasets, without having to process all of it.\n",
    "\n",
    "The most common actions are:\n",
    "- `count()`: count the number of elements in the RDD and return the result to the client.\n",
    "- `reduce(function)`: use a binary function to reduce all elements in the RDD to a single value, which is returned to the client.\n",
    "- `take(n)`: take the first n elements from the RDD and return them to the client.\n",
    "- `collect()`: collect the entire contents of the RDD and return them to the client (use with caution)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter the list for numbers <= 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Python\n",
    "filter(lambda n: n <= 10, python_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spark\n",
    "spark_rdd.filter(lambda n: n <= 10).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count the number of elements in the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Python\n",
    "len(python_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spark\n",
    "spark_rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count the odd numbers in the list (count the number of elements in the list filtered for odd numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Python\n",
    "len(filter(lambda n: n % 2 == 0, python_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spark\n",
    "spark_rdd.filter(lambda n: n % 2 == 0).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Take the square of each number after filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 4, 9, 16, 25, 36, 49, 64, 81, 100]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Python\n",
    "map(lambda n: n ** 2, filter(lambda n: n <= 10, python_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 4, 9, 16, 25, 36, 49, 64, 81, 100]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spark\n",
    "spark_rdd\\\n",
    ".filter(lambda n: n <= 10)\\\n",
    ".map(lambda n: n ** 2)\\\n",
    ".collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the number in two groups:\n",
    "- True: n < 50\n",
    "- False: n >= 50\n",
    "\n",
    "Then calculate the sum for each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(True, 1225), (False, 3725)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Python\n",
    "# Note the lowercase syntax\n",
    "from itertools import groupby\n",
    "\n",
    "map(lambda (key, values): (key, sum(values)),\n",
    "    groupby(python_list, lambda n: n < 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(False, 3725), (True, 1225)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spark\n",
    "# Note that instead of a groupBy and then sum, we just use keyBy and then reduceByKey,\n",
    "# which makes sure that the summing is a distributed operation,\n",
    "# thus the list for one group does not have to fit in memory.\n",
    "# Note the camelCase syntax\n",
    "spark_rdd\\\n",
    ".keyBy(lambda n: n < 50)\\\n",
    ".reduceByKey(lambda x,y: x + y)\\\n",
    ".collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the first 10 elements of the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Python\n",
    "python_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spark\n",
    "spark_rdd.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the sum of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4950"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Python\n",
    "sum(python_list) # Note that this is not parallelizable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4950"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spark\n",
    "# Using reduce() to make the distributed computation explicit ...\n",
    "spark_rdd.reduce(lambda x,y: x + y)\n",
    "# ... but spark_rdd.sum() will do as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lazy evaluation\n",
    "Spark RDDs are lazily evaluated. That means that no actual computation is performed until you request a result - Spark attempts to perform as little computation as possible for the required output. Thus, all of the transformation operations will only be executed when the result is requested by performing an action.\n",
    "\n",
    "Consider the example below. In both cases, we apply the same transformation to an RDD, but only in the second case we actually request the result. Notice that:\n",
    "1. Applying the transformation returns instantly.\n",
    "2. Applying a transformation and then taking 5 elements takes more time. Applying a time_consuming_function to all elements in the RDD should take 50 seconds (100 * 0.5), however the map is applied only to the first 5 elements as that is the number of elements we request: Spark refrains from performing unnecessary computations in order to produce the requested result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "def time_consuming_function(n):\n",
    "    sleep(0.5)      # sleep 0.5 seconds\n",
    "    return n ** 2   # return square of n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loops, best of 1: 13.8 Âµs per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n1 -r1\n",
    "\n",
    "# Transform the RDD using map, but take no action.\n",
    "result = spark_rdd.map(time_consuming_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 4, 9, 16]\n",
      "1 loops, best of 1: 2.54 s per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n1 -r1\n",
    "\n",
    "# Transform the RDD and take 5 elements. Print is required because the %%timeit otherwise clogs the output.\n",
    "result = spark_rdd.map(time_consuming_function)\n",
    "print result.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpointing and caching\n",
    "Spark RDDs are used to construct a DAG (directed acyclic graph) of operations. It is possible that machine failures occur while such a job is running. In this case, Spark will compute the results by re-running the parts of the DAG that were affected. While this effectively takes care of machine failures, it can lead to very expensive recomputation of intermediate results. Because of this, it sometimes makes sense to checkpoint an RDD, using the RDD's `checkpoint()` method.\n",
    "\n",
    "It is possible for an RDD to be the source for multiple operations, possibly many. In this case, reading the same RDD from disk for each iteration or operation can cause a considerable amount of overhead. In some cases the RDD might actually fit in the aggregated memory of the cluster that you are working on. In this case, it makes sense to cache the RDD in memory. Spark allows to do this, using the `cache()` method on an RDD."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
